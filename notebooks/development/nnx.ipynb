{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "939e3646-4f7f-4fe3-9ca5-ef731c16a46f",
   "metadata": {},
   "source": [
    "import flax.nnx as nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import tensorflow_datasets as tfds  # TFDS for MNIST\n",
    "import tensorflow as tf  # TensorFlow operations\n",
    "from functools import partial"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b774e5f-069b-4e7b-bb74-f5efafd03ba2",
   "metadata": {},
   "source": [
    "jax.devices()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0456627b-cace-4d89-9746-096bd141eef3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "tf.random.set_seed(0)  # set random seed for reproducibility\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "train_ds: tf.data.Dataset = tfds.load('mnist', split='train')\n",
    "test_ds: tf.data.Dataset = tfds.load('mnist', split='test')\n",
    "\n",
    "train_ds = train_ds.map(\n",
    "  lambda sample: {\n",
    "    'image': tf.cast(sample['image'], tf.float32) / 255,\n",
    "    'label': sample['label'],\n",
    "  }\n",
    ")  # normalize train set\n",
    "test_ds = test_ds.map(\n",
    "  lambda sample: {\n",
    "    'image': tf.cast(sample['image'], tf.float32) / 255,\n",
    "    'label': sample['label'],\n",
    "  }\n",
    ")  # normalize test set\n",
    "\n",
    "# create shuffled dataset by allocating a buffer size of 1024 to randomly draw elements from\n",
    "train_ds = train_ds.repeat(num_epochs).shuffle(1024)\n",
    "# group into batches of batch_size and skip incomplete batch, prefetch the next sample to improve latency\n",
    "train_ds = train_ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "# create shuffled dataset by allocating a buffer size of 1024 to randomly draw elements from\n",
    "test_ds = test_ds.shuffle(1024)\n",
    "# group into batches of batch_size and skip incomplete batch, prefetch the next sample to improve latency\n",
    "test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77aeb3d9-ded6-4026-992f-a4fb9933c3ca",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "class CNN(nnx.Module):\n",
    "  \"\"\"A simple CNN model.\"\"\"\n",
    "\n",
    "  def __init__(self, *, rngs: nnx.Rngs):\n",
    "    self.conv1 = nnx.Conv(1, 32, kernel_size=(3, 3), rngs=rngs)\n",
    "    self.conv2 = nnx.Conv(32, 64, kernel_size=(3, 3), rngs=rngs)\n",
    "    self.avg_pool = partial(nnx.avg_pool, window_shape=(2, 2), strides=(2, 2))\n",
    "    self.linear1 = nnx.Linear(3136, 256, rngs=rngs)\n",
    "    self.linear2 = nnx.Linear(256, 10, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.avg_pool(nnx.relu(self.conv1(x)))\n",
    "    x = self.avg_pool(nnx.relu(self.conv2(x)))\n",
    "    x = x.reshape(x.shape[0], -1)  # flatten\n",
    "    x = nnx.relu(self.linear1(x))\n",
    "    x = self.linear2(x)\n",
    "    return x\n",
    "\n",
    "model = CNN(rngs=nnx.Rngs(0))\n",
    "nnx.display(model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6721c1-d04f-4c11-b10e-217c4b328219",
   "metadata": {},
   "source": [
    "import jax.numpy as jnp  # JAX NumPy\n",
    "\n",
    "y = model(jnp.ones((1, 28, 28, 1)))\n",
    "nnx.display(y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb80b6a6-22ca-4369-9236-d489bc3bf289",
   "metadata": {},
   "source": [
    "import optax\n",
    "\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate, momentum))\n",
    "metrics = nnx.MultiMetric(\n",
    "  accuracy=nnx.metrics.Accuracy(), \n",
    "  loss=nnx.metrics.Average('loss'),\n",
    ")\n",
    "\n",
    "nnx.display(optimizer)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0673c1bc-3172-40a3-b3bd-9fa1d8cdcebc",
   "metadata": {},
   "source": [
    "def loss_fn(model: CNN, batch):\n",
    "  logits = model(batch['image'])\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "    logits=logits, labels=batch['label']\n",
    "  ).mean()\n",
    "  return loss, logits"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a99e564-9f3a-4e05-8a9b-1cb1e94a833f",
   "metadata": {},
   "source": [
    "@nnx.jit\n",
    "def train_step(model: CNN, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, logits), grads = grad_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch['label'])\n",
    "  optimizer.update(grads)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "358b2fcc-6bbe-4534-8014-da2a652ec637",
   "metadata": {},
   "source": [
    "@nnx.jit\n",
    "def eval_step(model: CNN, metrics: nnx.MultiMetric, batch):\n",
    "  loss, logits = loss_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch['label'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "828e7f0d-e88d-4576-8a5b-3d93f5c39b89",
   "metadata": {},
   "source": [
    "tf.random.set_seed(0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26985a91-b18e-4269-b025-b321c6d6f64a",
   "metadata": {},
   "source": [
    "num_steps_per_epoch = train_ds.cardinality().numpy() // num_epochs\n",
    "\n",
    "metrics_history = {\n",
    "  'train_loss': [],\n",
    "  'train_accuracy': [],\n",
    "  'test_loss': [],\n",
    "  'test_accuracy': [],\n",
    "}\n",
    "\n",
    "for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "  # Run the optimization for one step and make a stateful update to the following:\n",
    "  # - the train state's model parameters\n",
    "  # - the optimizer state\n",
    "  # - the training loss and accuracy batch metrics\n",
    "  train_step(model, optimizer, metrics, batch)\n",
    "\n",
    "  if (step + 1) % num_steps_per_epoch == 0:  # one training epoch has passed\n",
    "    # Log training metrics\n",
    "    for metric, value in metrics.compute().items():  # compute metrics\n",
    "      metrics_history[f'train_{metric}'].append(value)  # record metrics\n",
    "    metrics.reset()  # reset metrics for test set\n",
    "\n",
    "    # Compute metrics on the test set after each training epoch\n",
    "    for test_batch in test_ds.as_numpy_iterator():\n",
    "      eval_step(model, metrics, test_batch)\n",
    "\n",
    "    # Log test metrics\n",
    "    for metric, value in metrics.compute().items():\n",
    "      metrics_history[f'test_{metric}'].append(value)\n",
    "    metrics.reset()  # reset metrics for next training epoch\n",
    "\n",
    "    print(\n",
    "      f\"train epoch: {(step+1) // num_steps_per_epoch}, \"\n",
    "      f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "      f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\"\n",
    "    )\n",
    "    print(\n",
    "      f\"test epoch: {(step+1) // num_steps_per_epoch}, \"\n",
    "      f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
    "      f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\"\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cd19645-abd2-432e-a3f2-2c25fe04df5c",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt  # Visualization\n",
    "\n",
    "# Plot loss and accuracy in subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.set_title('Loss')\n",
    "ax2.set_title('Accuracy')\n",
    "for dataset in ('train', 'test'):\n",
    "  ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')\n",
    "  ax2.plot(metrics_history[f'{dataset}_accuracy'], label=f'{dataset}_accuracy')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cab63ca7-2a8f-44af-bc3c-b43a5d28fbc5",
   "metadata": {},
   "source": [
    "@nnx.jit\n",
    "def pred_step(model: CNN, batch):\n",
    "  logits = model(batch['image'])\n",
    "  return logits.argmax(axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d91ddc4-31f1-4184-b3ae-527b08790300",
   "metadata": {},
   "source": [
    "test_batch = test_ds.as_numpy_iterator().next()\n",
    "pred = pred_step(model, test_batch)\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(12, 12))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "  ax.imshow(test_batch['image'][i, ..., 0], cmap='gray')\n",
    "  ax.set_title(f'label={pred[i]}')\n",
    "  ax.axis('off')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc6a339-4eb9-4d35-adb7-c3ca856fefa9",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b45668-efaa-48a9-a65c-24313000c096",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physnetjax",
   "language": "python",
   "name": "jaxphyscharmm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
